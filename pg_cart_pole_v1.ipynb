{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt\n!apt install -y python-opengl ffmpeg xvfb\n!pip install pyvirtualdisplay pyglet==1.5.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-05-08T14:34:48.146058Z","iopub.execute_input":"2024-05-08T14:34:48.146817Z","iopub.status.idle":"2024-05-08T14:35:36.171538Z","shell.execute_reply.started":"2024-05-08T14:34:48.146786Z","shell.execute_reply":"2024-05-08T14:35:36.170380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport datetime\nimport json\nimport os\nimport tempfile\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\nimport gym\n\nfrom huggingface_hub import notebook_login, HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nimport imageio\nfrom pathlib import Path\nfrom pyvirtualdisplay import Display","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:35:36.173624Z","iopub.execute_input":"2024-05-08T14:35:36.173925Z","iopub.status.idle":"2024-05-08T14:35:40.286338Z","shell.execute_reply.started":"2024-05-08T14:35:36.173898Z","shell.execute_reply":"2024-05-08T14:35:40.285525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:35:40.287450Z","iopub.execute_input":"2024-05-08T14:35:40.287821Z","iopub.status.idle":"2024-05-08T14:35:40.295436Z","shell.execute_reply.started":"2024-05-08T14:35:40.287795Z","shell.execute_reply":"2024-05-08T14:35:40.294418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    scores_deque = deque(maxlen=100)\n    scores = []\n    for i_episode in range(1, n_training_episodes + 1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()[0]\n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _, _ = env.step(action)\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n\n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n       \n        for t in range(n_steps)[::-1]:\n            disc_return_t = returns[0] if len(returns) > 0 else 0\n            returns.appendleft(gamma * disc_return_t + rewards[t])\n\n        eps = np.finfo(np.float32).eps.item()\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        if i_episode % print_every == 0:\n            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n\n    return scores\n\n\ndef evaluate_agent(env, max_steps, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The Reinforce agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()[0]\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        for step in range(max_steps):\n            action, _ = policy.act(state)\n            new_state, reward, done, info, _ = env.step(action)\n            total_rewards_ep += reward\n\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward\n\n\ndef record_video(env, policy, out_directory, fps=30):\n    \"\"\"\n    Generate a replay video of the agent\n    :param env\n    :param out_directory\n    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n    \"\"\"\n    images = []\n    done = False\n    state = env.reset()[0]\n    img = env.render()\n    images.append(img)\n    while not done:\n        # Take the action (index) that have the maximum expected future reward given that state\n        action, _ = policy.act(state)\n        state, reward, done, info, _ = env.step(action)  # We directly put next_state = state for recording logic\n        img = env.render()\n        images.append(img)\n        \n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n    \n\ndef push_to_hub(repo_id,\n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the Hub\n\n  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n  :param model: the pytorch model we want to save\n  :param hyperparameters: training hyperparameters\n  :param eval_env: evaluation environment\n  :param video_fps: how many frame per seconds to record our video replay\n  \"\"\"\n\n  _, repo_name = repo_id.split(\"/\")\n  api = HfApi()\n\n  # Step 1: Create the repo\n  repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n  )\n\n  with tempfile.TemporaryDirectory() as tmpdirname:\n    local_directory = Path(tmpdirname)\n\n    # Step 2: Save the model\n    torch.save(model, local_directory / \"model.pt\")\n\n    # Step 3: Save the hyperparameters to JSON\n    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n      json.dump(hyperparameters, outfile)\n\n    # Step 4: Evaluate the model and build JSON\n    mean_reward, std_reward = evaluate_agent(eval_env,\n                                            hyperparameters[\"max_t\"],\n                                            hyperparameters[\"n_evaluation_episodes\"],\n                                            model)\n    # Get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n          \"env_id\": hyperparameters[\"env_id\"],\n          \"mean_reward\": mean_reward,\n          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n          \"eval_datetime\": eval_form_datetime,\n    }\n\n    # Write a JSON file\n    with open(local_directory / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = hyperparameters[\"env_id\"]\n\n    metadata = {}\n    metadata[\"tags\"] = [\n          env_name,\n          \"reinforce\",\n          \"reinforcement-learning\",\n          \"custom-implementation\",\n          \"deep-rl-class\"\n      ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n      )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n      # **Reinforce** Agent playing **{env_id}**\n      This is a trained model of a **Reinforce** agent playing **{env_id}** .\n      To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n      \"\"\"\n\n    readme_path = local_directory / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n          readme = f.read()\n    else:\n      readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n      f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    #video_path =  local_directory / \"replay.mp4\"\n    #record_video(env, model, video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n          repo_id=repo_id,\n          folder_path=local_directory,\n          path_in_repo=\".\",\n    )\n\n    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")    ","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:35:40.299465Z","iopub.execute_input":"2024-05-08T14:35:40.299814Z","iopub.status.idle":"2024-05-08T14:35:40.706399Z","shell.execute_reply.started":"2024-05-08T14:35:40.299783Z","shell.execute_reply":"2024-05-08T14:35:40.705447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"virtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:35:40.707617Z","iopub.execute_input":"2024-05-08T14:35:40.708034Z","iopub.status.idle":"2024-05-08T14:35:41.079100Z","shell.execute_reply.started":"2024-05-08T14:35:40.707985Z","shell.execute_reply":"2024-05-08T14:35:41.078102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env_id = \"CartPole-v1\"\nenv = gym.make(env_id, render_mode=\"rgb_array\")\n\neval_env = gym.make(env_id, render_mode=\"rgb_array\")\n\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:35:41.080467Z","iopub.execute_input":"2024-05-08T14:35:41.080911Z","iopub.status.idle":"2024-05-08T14:35:41.095748Z","shell.execute_reply.started":"2024-05-08T14:35:41.080879Z","shell.execute_reply":"2024-05-08T14:35:41.094907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PARAMS = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:35:41.096910Z","iopub.execute_input":"2024-05-08T14:35:41.097286Z","iopub.status.idle":"2024-05-08T14:35:41.102414Z","shell.execute_reply.started":"2024-05-08T14:35:41.097254Z","shell.execute_reply":"2024-05-08T14:35:41.101476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nPOLICY = Policy(\n    PARAMS[\"state_space\"],\n    PARAMS[\"action_space\"],\n    PARAMS[\"h_size\"],\n).to(DEVICE)\n\nOPTIMIZER = optim.Adam(POLICY.parameters(), lr=PARAMS[\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:35:41.103809Z","iopub.execute_input":"2024-05-08T14:35:41.104414Z","iopub.status.idle":"2024-05-08T14:35:43.282487Z","shell.execute_reply.started":"2024-05-08T14:35:41.104381Z","shell.execute_reply":"2024-05-08T14:35:43.281518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = reinforce(\n    POLICY,\n    OPTIMIZER,\n    PARAMS[\"n_training_episodes\"],\n    PARAMS[\"max_t\"],\n    PARAMS[\"gamma\"],\n    100,\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-08T14:35:43.283714Z","iopub.execute_input":"2024-05-08T14:35:43.284092Z","iopub.status.idle":"2024-05-08T14:46:38.413994Z","shell.execute_reply.started":"2024-05-08T14:35:43.284067Z","shell.execute_reply":"2024-05-08T14:46:38.413057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_agent(\n    eval_env\n    , PARAMS[\"max_t\"]\n    , PARAMS[\"n_evaluation_episodes\"]\n    , POLICY\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:46:38.421369Z","iopub.execute_input":"2024-05-08T14:46:38.421768Z","iopub.status.idle":"2024-05-08T14:46:44.852499Z","shell.execute_reply.started":"2024-05-08T14:46:38.421741Z","shell.execute_reply":"2024-05-08T14:46:44.851596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:46:44.853538Z","iopub.execute_input":"2024-05-08T14:46:44.853803Z","iopub.status.idle":"2024-05-08T14:46:44.875355Z","shell.execute_reply.started":"2024-05-08T14:46:44.853779Z","shell.execute_reply":"2024-05-08T14:46:44.874165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"repo_id = \"jaymanvirk/rl_cart_pole_v1\"\npush_to_hub(\n    repo_id,\n    POLICY, \n    PARAMS,  \n    eval_env,  \n    video_fps=30\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T14:47:10.954846Z","iopub.execute_input":"2024-05-08T14:47:10.955521Z","iopub.status.idle":"2024-05-08T14:50:11.309423Z","shell.execute_reply.started":"2024-05-08T14:47:10.955486Z","shell.execute_reply":"2024-05-08T14:50:11.308016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}