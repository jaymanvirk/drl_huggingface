{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt install -y python-opengl ffmpeg xvfb\n\n!pip install pyglet==1.5 pyvirtualdisplay gym==0.22 imageio-ffmpeg huggingface_hub gym[box2d]==0.22","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyvirtualdisplay import Display\n\nimport os\nimport random\nimport time\nfrom distutils.util import strtobool\n\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions.categorical import Categorical\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom huggingface_hub import HfApi, upload_folder, notebook_login\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\n\nmsg = Printer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"virtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingArguments:\n    def __init__(self, **kwargs):\n        self.exp_name = kwargs.get(\"exp_name\", os.path.basename(__file__).rstrip(\".py\"))\n        self.seed = kwargs.get(\"seed\", 0)\n        self.torch_deterministic = kwargs.get(\"torch_deterministic\", True)\n        self.cuda = kwargs.get(\"cuda\", True)\n        self.track = kwargs.get(\"track\", False)\n        self.wandb_project_name = kwargs.get(\"wandb_project_name\", \"cleanRL\")\n        self.wandb_entity = kwargs.get(\"wandb_entity\", None)\n        self.capture_video = kwargs.get(\"capture_video\", False)\n\n        # Algorithm specific arguments\n        self.env_id = kwargs.get(\"env_id\", None)\n        self.total_timesteps = kwargs.get(\"total_timesteps\", 50000)\n        self.learning_rate = kwargs.get(\"learning_rate\", 2.5e-4)\n        self.num_envs = kwargs.get(\"num_envs\", 4)\n        self.num_steps = kwargs.get(\"num_steps\", 128)\n        self.anneal_lr = kwargs.get(\"anneal_lr\", True)\n        self.gae = kwargs.get(\"gae\", True)\n        self.gamma = kwargs.get(\"gamma\", 0.99)\n        self.gae_lambda = kwargs.get(\"gae_lambda\", 0.95)\n        self.num_minibatches = kwargs.get(\"num_minibatches\", 4)\n        self.update_epochs = kwargs.get(\"update_epochs\", 4)\n        self.norm_adv = kwargs.get(\"norm_adv\", True)\n        self.clip_coef = kwargs.get(\"clip_coef\", 0.2)\n        self.clip_vloss = kwargs.get(\"clip_vloss\", True)\n        self.ent_coef = kwargs.get(\"ent_coef\", 0.01)\n        self.vf_coef = kwargs.get(\"vf_coef\", 0.5)\n        self.max_grad_norm = kwargs.get(\"max_grad_norm\", 0.5)\n        self.target_kl = kwargs.get(\"target_kl\", None)\n\n        self.batch_size = self.num_envs * self.num_steps\n        self.minibatch_size = self.batch_size // self.num_minibatches","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"initial commit\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n\n        # Step 3: Evaluate the model and build JSON\n        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n\n        # First get datetime\n        eval_datetime = datetime.datetime.now()\n        eval_form_datetime = eval_datetime.isoformat()\n\n        evaluate_data = {\n            \"env_id\": hyperparameters.env_id,\n            \"mean_reward\": mean_reward,\n            \"std_reward\": std_reward,\n            \"n_evaluation_episodes\": 10,\n            \"eval_datetime\": eval_form_datetime,\n        }\n\n        # Write a JSON file\n        with open(tmpdirname / \"results.json\", \"w\") as outfile:\n            json.dump(evaluate_data, outfile)\n\n        # Step 4: Generate a video\n        video_path = tmpdirname / \"replay.mp4\"\n        record_video(eval_env, model, video_path, video_fps)\n\n        # Step 5: Generate the model card\n        generated_model_card, metadata = _generate_model_card(\n            \"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters\n        )\n        _save_model_card(tmpdirname, generated_model_card, metadata)\n\n        # Step 6: Add logs if needed\n        if logs:\n            _add_logdir(tmpdirname, Path(logs))\n\n        msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n\n        repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n        msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n    return repo_url","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _evaluate_agent(env, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        while done is False:\n            state = torch.Tensor(state).to(device)\n            action, _, _, _ = policy.get_action_and_value(state)\n            new_state, reward, done, info = env.step(action.cpu().numpy())\n            total_rewards_ep += reward\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def record_video(env, policy, out_directory, fps=30):\n    images = []\n    done = False\n    state = env.reset()\n    img = env.render(mode=\"rgb_array\")\n    images.append(img)\n    while not done:\n        state = torch.Tensor(state).to(device)\n        # Take the action (index) that have the maximum expected future reward given that state\n        action, _, _, _ = policy.get_action_and_value(state)\n        state, reward, done, info = env.step(\n            action.cpu().numpy()\n        )  # We directly put next_state = state for recording logic\n        img = env.render(mode=\"rgb_array\")\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n    \"\"\"\n    Generate the model card for the Hub\n    :param model_name: name of the model\n    :env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    :hyperparameters: training arguments\n    \"\"\"\n    # Step 1: Select the tags\n    metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n    # Transform the hyperparams namespace to string\n    converted_dict = vars(hyperparameters)\n    converted_str = str(converted_dict)\n    converted_str = converted_str.split(\", \")\n    converted_str = \"\\n\".join(converted_str)\n\n    # Step 2: Generate the model card\n    model_card = f\"\"\"\n  # PPO Agent Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n\n  # Hyperparameters\n  \n  {converted_str}\n  \"\"\"\n    return model_card, metadata","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_metadata(model_name, env_id, mean_reward, std_reward):\n    \"\"\"\n    Define the tags for the model card\n    :param model_name: name of the model\n    :param env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    \"\"\"\n    metadata = {}\n    metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\",\n    ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=model_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_id,\n        dataset_id=env_id,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    return metadata","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _add_logdir(local_path: Path, logdir: Path):\n    \"\"\"Adds a logdir to the repository.\n    :param local_path: repository directory\n    :param logdir: logdir directory\n    \"\"\"\n    if logdir.exists() and logdir.is_dir():\n        # Add the logdir to the repository under new dir called logs\n        repo_logdir = local_path / \"logs\"\n\n        # Delete current logs if they exist\n        if repo_logdir.exists():\n            shutil.rmtree(repo_logdir)\n\n        # Copy logdir into repo logdir\n        shutil.copytree(logdir, repo_logdir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_env(env_id, seed, idx, capture_video, run_name):\n    def thunk():\n        env = gym.make(env_id)\n        env = gym.wrappers.RecordEpisodeStatistics(env)\n        if capture_video:\n            if idx == 0:\n                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n        env.seed(seed)\n        env.action_space.seed(seed)\n        env.observation_space.seed(seed)\n        return env\n\n    return thunk","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Agent(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 1), std=1.0),\n        )\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n        )\n        \n    def get_value(self, x):\n        return self.critic(x)\n\n    def get_action_and_value(self, x, action=None):\n        logits = self.actor(x)\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_trained_model(args):\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    if args.track:\n        import wandb\n\n        wandb.init(\n            project=args.wandb_project_name,\n            entity=args.wandb_entity,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    writer = SummaryWriter(f\"runs/{run_name}\")\n    writer.add_text(\n        \"hyperparameters\",\n        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n    )\n\n    # TRY NOT TO MODIFY: seeding\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n\n    # env setup\n    envs = gym.vector.SyncVectorEnv(\n        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n    )\n    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n\n    agent = Agent(envs).to(device)\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n\n    # ALGO Logic: Storage setup\n    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n\n    # TRY NOT TO MODIFY: start the game\n    global_step = 0\n    start_time = time.time()\n    next_obs = torch.Tensor(envs.reset()).to(device)\n    next_done = torch.zeros(args.num_envs).to(device)\n    num_updates = args.total_timesteps // args.batch_size\n\n    for update in range(1, num_updates + 1):\n        # Annealing the rate if instructed to do so.\n        if args.anneal_lr:\n            frac = 1.0 - (update - 1.0) / num_updates\n            lrnow = frac * args.learning_rate\n            optimizer.param_groups[0][\"lr\"] = lrnow\n\n        for step in range(0, args.num_steps):\n            global_step += 1 * args.num_envs\n            obs[step] = next_obs\n            dones[step] = next_done\n\n            # ALGO LOGIC: action logic\n            with torch.no_grad():\n                action, logprob, _, value = agent.get_action_and_value(next_obs)\n                values[step] = value.flatten()\n            actions[step] = action\n            logprobs[step] = logprob\n\n            # TRY NOT TO MODIFY: execute the game and log data.\n            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n            rewards[step] = torch.tensor(reward).to(device).view(-1)\n            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n\n            for item in info:\n                if \"episode\" in item.keys():\n                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n                    break\n\n        # bootstrap value if not done\n        with torch.no_grad():\n            next_value = agent.get_value(next_obs).reshape(1, -1)\n            if args.gae:\n                advantages = torch.zeros_like(rewards).to(device)\n                lastgaelam = 0\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        nextvalues = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        nextvalues = values[t + 1]\n                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n                returns = advantages + values\n            else:\n                returns = torch.zeros_like(rewards).to(device)\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        next_return = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        next_return = returns[t + 1]\n                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n                advantages = returns - values\n\n        # flatten the batch\n        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n        b_logprobs = logprobs.reshape(-1)\n        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n        b_advantages = advantages.reshape(-1)\n        b_returns = returns.reshape(-1)\n        b_values = values.reshape(-1)\n\n        # Optimizing the policy and value network\n        b_inds = np.arange(args.batch_size)\n        clipfracs = []\n        for epoch in range(args.update_epochs):\n            np.random.shuffle(b_inds)\n            for start in range(0, args.batch_size, args.minibatch_size):\n                end = start + args.minibatch_size\n                mb_inds = b_inds[start:end]\n\n                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n                    b_obs[mb_inds], b_actions.long()[mb_inds]\n                )\n                logratio = newlogprob - b_logprobs[mb_inds]\n                ratio = logratio.exp()\n\n                with torch.no_grad():\n                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n                    old_approx_kl = (-logratio).mean()\n                    approx_kl = ((ratio - 1) - logratio).mean()\n                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n\n                mb_advantages = b_advantages[mb_inds]\n                if args.norm_adv:\n                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n\n                # Policy loss\n                pg_loss1 = -mb_advantages * ratio\n                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n\n                # Value loss\n                newvalue = newvalue.view(-1)\n                if args.clip_vloss:\n                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n                    v_clipped = b_values[mb_inds] + torch.clamp(\n                        newvalue - b_values[mb_inds],\n                        -args.clip_coef,\n                        args.clip_coef,\n                    )\n                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n                    v_loss = 0.5 * v_loss_max.mean()\n                else:\n                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n\n                entropy_loss = entropy.mean()\n                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n                optimizer.step()\n\n            if args.target_kl is not None:\n                if approx_kl > args.target_kl:\n                    break\n\n        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n        var_y = np.var(y_true)\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n\n        # TRY NOT TO MODIFY: record rewards for plotting purposes\n        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n\n    envs.close()\n    writer.close()\n    \n    return agent, run_name","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    env_id = \"LunarLander-v2\"\n    , total_timesteps = 1e5\n    , num_envs = 8\n    , num_steps = 256\n    , learning_rate = 1e-3\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agent, run_name = get_trained_model(args)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"repo_id = \"jaymanvirk/ppo_cleanrl_lunar_lander_v2\"\npackage_to_hub(\n        repo_id = repo_id\n        , model = get_trained_model(args)\n        , hyperparameters = args\n        , eval_env = gym.make(args.env_id)\n        , logs=f\"runs/{run_name}\"\n    )","metadata":{},"execution_count":null,"outputs":[]}]}